{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "XJdv7AznYyhj",
        "outputId": "79328147-a87b-430e-8009-10d465334404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "Kaggle executable path: /usr/local/bin/kaggle\n",
            "Dataset downloaded to: ./Datasets/asl-alphabet\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# 2) Starting in the objectives implementation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"CV_project.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1-U7aARa5bBUd9Mvd1rOAPp9UnAc8XKgj\n",
        "\n",
        "## **1) Downloading the 2 datasets that we will compare between**\n",
        "\"\"\"\n",
        "\n",
        "!pip install kaggle\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Find the kaggle executable path\n",
        "kaggle_path = subprocess.check_output(['which', 'kaggle']).decode().strip()\n",
        "print(f\"Kaggle executable path: {kaggle_path}\")\n",
        "\n",
        "def download_kaggle_dataset(dataset_name, output_path):\n",
        "    \"\"\"Downloads a Kaggle dataset.\"\"\"\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    # Use the updated kaggle path\n",
        "    command = f\"{kaggle_path} datasets download -d {dataset_name} -p {output_path} --unzip\"\n",
        "    subprocess.run(command, shell=True, check=True)\n",
        "    print(f\"Dataset downloaded to: {output_path}\")\n",
        "\n",
        "dataset_name = \"grassknoted/asl-alphabet\"\n",
        "output_path = \"./Datasets/asl-alphabet\"\n",
        "download_kaggle_dataset(dataset_name, output_path)\n",
        "\n",
        "\"\"\"# 2) Starting in the objectives implementation\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check the contents of the directory where the dataset was downloaded\n",
        "output_path = './Datasets/asl-alphabet'\n",
        "print(os.listdir(output_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3VvhPcxHm9h",
        "outputId": "1fa36ad6-1a2b-4f1d-d9a5-c96678aa5f0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['asl_alphabet_test', 'asl_alphabet_train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the contents of the training dataset\n",
        "train_path = os.path.join(output_path, 'asl_alphabet_train')\n",
        "print(\"Training Subdirectory Contents:\")\n",
        "print(os.listdir(train_path))\n",
        "\n",
        "# List the contents of the test dataset\n",
        "test_path = os.path.join(output_path, 'asl_alphabet_test')\n",
        "print(\"Test Subdirectory Contents:\")\n",
        "print(os.listdir(test_path))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIU7n3xUXdDx",
        "outputId": "af517220-3356-476f-94fc-362472573558"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Subdirectory Contents:\n",
            "['asl_alphabet_train']\n",
            "Test Subdirectory Contents:\n",
            "['asl_alphabet_test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nested_train_path = os.path.join(train_path, 'asl_alphabet_train')\n",
        "print(\"Contents of the Nested Training Subdirectory:\")\n",
        "print(os.listdir(nested_train_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDyR2XIeXmRw",
        "outputId": "7ca1b558-6682-4e49-eeaf-e355755cb19b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the Nested Training Subdirectory:\n",
            "['P', 'Z', 'D', 'del', 'F', 'I', 'R', 'X', 'K', 'S', 'J', 'nothing', 'T', 'Y', 'space', 'V', 'C', 'B', 'E', 'G', 'U', 'H', 'Q', 'N', 'A', 'M', 'L', 'O', 'W']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Test Dataset Structure\n",
        "print(\"\\nChecking Test Dataset Structure...\")\n",
        "nested_test_path = os.path.join(test_path, 'asl_alphabet_test')\n",
        "if os.path.exists(nested_test_path):\n",
        "    print(\"Contents of the Nested Test Subdirectory:\")\n",
        "    print(os.listdir(nested_test_path))\n",
        "    test_path = nested_test_path  # Update to the nested path\n",
        "else:\n",
        "    print(\"Test Subdirectory Contents:\")\n",
        "    print(os.listdir(test_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epnZMUNxX4ms",
        "outputId": "9fa2e95d-33eb-4e89-dbb9-49f5858f5dc2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking Test Dataset Structure...\n",
            "Contents of the Nested Test Subdirectory:\n",
            "['C_test.jpg', 'B_test.jpg', 'J_test.jpg', 'N_test.jpg', 'R_test.jpg', 'V_test.jpg', 'Q_test.jpg', 'L_test.jpg', 'K_test.jpg', 'W_test.jpg', 'nothing_test.jpg', 'D_test.jpg', 'S_test.jpg', 'X_test.jpg', 'space_test.jpg', 'F_test.jpg', 'Y_test.jpg', 'I_test.jpg', 'E_test.jpg', 'H_test.jpg', 'M_test.jpg', 'T_test.jpg', 'G_test.jpg', 'U_test.jpg', 'P_test.jpg', 'Z_test.jpg', 'O_test.jpg', 'A_test.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the dataset paths\n",
        "output_path = './Datasets/asl-alphabet'\n",
        "\n",
        "# Check if the main dataset folder exists\n",
        "if os.path.exists(output_path):\n",
        "    print(f\"The dataset folder exists at: {output_path}\")\n",
        "else:\n",
        "    print(\"Dataset folder does not exist. Please check the download process.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N1yFECUH425",
        "outputId": "ef1c4266-20dc-4d4c-9bea-1ccfaa4fa96c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset folder exists at: ./Datasets/asl-alphabet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the subdirectories\n",
        "train_path = os.path.join(output_path, 'asl_alphabet_train')\n",
        "test_path = os.path.join(output_path, 'asl_alphabet_test')\n",
        "\n",
        "# Check if the training folder exists and list its contents\n",
        "if os.path.exists(train_path):\n",
        "    print(\"\\nTraining Directory Contents:\")\n",
        "    print(os.listdir(train_path))\n",
        "else:\n",
        "    print(\"Training directory not found.\")\n",
        "\n",
        "# Check if the test folder exists and list its contents\n",
        "if os.path.exists(test_path):\n",
        "    print(\"\\nTest Directory Contents:\")\n",
        "    print(os.listdir(test_path))\n",
        "else:\n",
        "    print(\"Test directory not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEUKZNzgZm8b",
        "outputId": "f12a58df-5dae-4382-d215-28026d11b8f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Directory Contents:\n",
            "['asl_alphabet_train']\n",
            "\n",
            "Test Directory Contents:\n",
            "['asl_alphabet_test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the contents of the directory after unzipping\n",
        "output_path = './Datasets/asl-alphabet'\n",
        "print(os.listdir(output_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAeOWgJlI3k-",
        "outputId": "200c5ebb-d75c-4e0c-fe64-7c6f25dd2568"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['asl_alphabet_test', 'asl_alphabet_train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract 21 key points (landmarks) for each hand, analyze to know shape and size (not sure if needed)\n",
        "!pip install mediapipe opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR9WvivNbSh6",
        "outputId": "258c1549-a805-485c-c10f-235a4c7dde4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.20)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "eSSH2rykb6pT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize MediaPipe Hands module\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands()\n",
        "\n",
        "# Function to extract hand landmarks\n",
        "def extract_hand_landmarks(image):\n",
        "    # Convert image to RGB (MediaPipe requires RGB input)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    result = hands.process(image_rgb)\n",
        "\n",
        "    landmarks = []\n",
        "    if result.multi_hand_landmarks:\n",
        "        for hand_landmarks in result.multi_hand_landmarks:\n",
        "            for landmark in hand_landmarks.landmark:\n",
        "                landmarks.append([landmark.x, landmark.y, landmark.z])  # x, y, z coordinates\n",
        "    return landmarks\n",
        "\n",
        "# Function to calculate Euclidean distance between two points\n",
        "def calculate_distance(point1, point2):\n",
        "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
        "\n",
        "# Extract minimal features (Hand Size, Finger Lengths, Orientation)\n",
        "def extract_minimal_features(image):\n",
        "    landmarks = extract_hand_landmarks(image)\n",
        "    if not landmarks:\n",
        "        return None  # No hand detected\n",
        "\n",
        "    # 1. Hand Size (Bounding Box)\n",
        "    x_coords = [landmark[0] for landmark in landmarks]\n",
        "    y_coords = [landmark[1] for landmark in landmarks]\n",
        "    x_min, x_max = min(x_coords), max(x_coords)\n",
        "    y_min, y_max = min(y_coords), max(y_coords)\n",
        "    hand_width = x_max - x_min\n",
        "    hand_height = y_max - y_min\n",
        "\n",
        "    # 2. Finger Lengths (Wrist to Fingertip)\n",
        "    wrist = landmarks[0]  # Wrist (landmark 0)\n",
        "    thumb_tip = landmarks[4]  # Thumb tip (landmark 4)\n",
        "    index_tip = landmarks[8]  # Index finger tip (landmark 8)\n",
        "    middle_tip = landmarks[12]  # Middle finger tip (landmark 12)\n",
        "    ring_tip = landmarks[16]  # Ring finger tip (landmark 16)\n",
        "    pinky_tip = landmarks[20]  # Pinky finger tip (landmark 20)\n",
        "\n",
        "    # Calculate lengths of fingers\n",
        "    thumb_length = calculate_distance(wrist, thumb_tip)\n",
        "    index_length = calculate_distance(wrist, index_tip)\n",
        "    middle_length = calculate_distance(wrist, middle_tip)\n",
        "    ring_length = calculate_distance(wrist, ring_tip)\n",
        "    pinky_length = calculate_distance(wrist, pinky_tip)\n",
        "\n",
        "    # 3. Hand Orientation (angle between wrist, index, and middle tip)\n",
        "    def calculate_angle(p1, p2, p3):\n",
        "        v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])\n",
        "        v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n",
        "        dot_product = np.dot(v1, v2)\n",
        "        magnitude_v1 = np.linalg.norm(v1)\n",
        "        magnitude_v2 = np.linalg.norm(v2)\n",
        "        cos_theta = dot_product / (magnitude_v1 * magnitude_v2)\n",
        "        angle = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
        "        return np.degrees(angle)\n",
        "\n",
        "    # Calculate orientation between wrist, index, and middle fingers\n",
        "    orientation = calculate_angle(wrist, index_tip, middle_tip)\n",
        "\n",
        "    # Create and return the features dictionary\n",
        "    features = {\n",
        "        \"hand_width\": hand_width,\n",
        "        \"hand_height\": hand_height,\n",
        "        \"thumb_length\": thumb_length,\n",
        "        \"index_length\": index_length,\n",
        "        \"middle_length\": middle_length,\n",
        "        \"ring_length\": ring_length,\n",
        "        \"pinky_length\": pinky_length,\n",
        "        \"orientation\": orientation\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "bQtuhnoZcCrJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Define the training dataset path\n",
        "train_path = './Datasets/asl-alphabet/asl_alphabet_train/asl_alphabet_train'\n",
        "\n",
        "# Function to load images and labels in batches\n",
        "def load_images_and_labels_in_batches(data_path, batch_size=1000, size=(200, 200)):\n",
        "    \"\"\"Load and preprocess images in batches.\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    batch_images = []\n",
        "    batch_labels = []\n",
        "\n",
        "    print(f\"Scanning directory: {data_path}\")\n",
        "    for folder_name in os.listdir(data_path):\n",
        "        folder_path = os.path.join(data_path, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            print(f\"Processing folder: {folder_name}\")\n",
        "            for file_name in os.listdir(folder_path):\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                if file_name.endswith(('.jpg', '.png', '.jpeg')):  # Ensure valid image files\n",
        "                    image = cv2.imread(file_path)\n",
        "                    if image is not None:\n",
        "                        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "                        resized_image = cv2.resize(gray_image, size)         # Resize the image\n",
        "                        batch_images.append(resized_image)\n",
        "                        batch_labels.append(folder_name)\n",
        "\n",
        "                        # When batch size is reached, save the batch and reset\n",
        "                        if len(batch_images) == batch_size:\n",
        "                            images.append(np.array(batch_images))\n",
        "                            labels.append(np.array(batch_labels))\n",
        "                            batch_images = []\n",
        "                            batch_labels = []\n",
        "\n",
        "    # Add any remaining images if the last batch is incomplete\n",
        "    if batch_images:\n",
        "        images.append(np.array(batch_images))\n",
        "        labels.append(np.array(batch_labels))\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "# Example: Load training images in batches\n",
        "batch_size = 500\n",
        "train_batches, train_label_batches = load_images_and_labels_in_batches(train_path, batch_size)\n",
        "print(f\"Loaded {len(train_batches)} batches of training data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnjOupjdxrJZ",
        "outputId": "0bfbc503-a2fd-4a48-9def-2611395fc2e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning directory: ./Datasets/asl-alphabet/asl_alphabet_train/asl_alphabet_train\n",
            "Processing folder: P\n",
            "Processing folder: Z\n",
            "Processing folder: D\n",
            "Processing folder: del\n",
            "Processing folder: F\n",
            "Processing folder: I\n",
            "Processing folder: R\n",
            "Processing folder: X\n",
            "Processing folder: K\n",
            "Processing folder: S\n",
            "Processing folder: J\n",
            "Processing folder: nothing\n",
            "Processing folder: T\n",
            "Processing folder: Y\n",
            "Processing folder: space\n",
            "Processing folder: V\n",
            "Processing folder: C\n",
            "Processing folder: B\n",
            "Processing folder: E\n",
            "Processing folder: G\n",
            "Processing folder: U\n",
            "Processing folder: H\n",
            "Processing folder: Q\n",
            "Processing folder: N\n",
            "Processing folder: A\n",
            "Processing folder: M\n",
            "Processing folder: L\n",
            "Processing folder: O\n",
            "Processing folder: W\n",
            "Loaded 174 batches of training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batches(batches, size=(200, 200)):\n",
        "    \"\"\"Preprocess images in batches: Resize and convert to grayscale.\"\"\"\n",
        "    processed_batches = []\n",
        "    for batch in batches:\n",
        "        processed_batch = []\n",
        "        for image in batch:\n",
        "            # Check if the image is already grayscale\n",
        "            if len(image.shape) == 3:  # 3 channels (RGB/BGR)\n",
        "                gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            else:  # Already grayscale\n",
        "                gray_image = image\n",
        "\n",
        "            # Resize the image\n",
        "            resized_image = cv2.resize(gray_image, size)\n",
        "            processed_batch.append(resized_image)\n",
        "\n",
        "        processed_batches.append(np.array(processed_batch))\n",
        "    return processed_batches\n"
      ],
      "metadata": {
        "id": "kAgYmIeFxAv7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of batches\n",
        "print(f\"Number of Batches: {len(train_batches)}\")\n",
        "\n",
        "# Check the shape of the first batch\n",
        "print(f\"Shape of First Batch (Images): {train_batches[0].shape}\")\n",
        "print(f\"Number of Labels in First Batch: {len(train_label_batches[0])}\")\n",
        "\n",
        "# Check a few labels from the first batch\n",
        "print(f\"Labels in First Batch: {train_label_batches[0][:10]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KQu0B9by4bt",
        "outputId": "d03530f6-b8f6-46ee-b163-7a97e99196c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Batches: 174\n",
            "Shape of First Batch (Images): (500, 200, 200)\n",
            "Number of Labels in First Batch: 500\n",
            "Labels in First Batch: ['P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P' 'P']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Directory to save normalized batches\n",
        "normalized_batch_dir = \"./normalized_batches/\"\n",
        "os.makedirs(normalized_batch_dir, exist_ok=True)\n",
        "\n",
        "# Normalize and save batches one at a time\n",
        "for i, batch in enumerate(train_batches):\n",
        "    # Normalize the batch\n",
        "    normalized_batch = batch / 255.0\n",
        "\n",
        "    # Save the batch to disk as a NumPy file\n",
        "    batch_file = os.path.join(normalized_batch_dir, f\"batch_{i}.npy\")\n",
        "    np.save(batch_file, normalized_batch)\n",
        "\n",
        "    # Clear memory for the processed batch\n",
        "    del normalized_batch\n",
        "\n",
        "    print(f\"Processed and saved batch {i + 1} of {len(train_batches)}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGmSSq2v0QlE",
        "outputId": "d1a8cffe-8286-48fc-dc7c-324250f0db7f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved batch 1 of 174.\n",
            "Processed and saved batch 2 of 174.\n",
            "Processed and saved batch 3 of 174.\n",
            "Processed and saved batch 4 of 174.\n",
            "Processed and saved batch 5 of 174.\n",
            "Processed and saved batch 6 of 174.\n",
            "Processed and saved batch 7 of 174.\n",
            "Processed and saved batch 8 of 174.\n",
            "Processed and saved batch 9 of 174.\n",
            "Processed and saved batch 10 of 174.\n",
            "Processed and saved batch 11 of 174.\n",
            "Processed and saved batch 12 of 174.\n",
            "Processed and saved batch 13 of 174.\n",
            "Processed and saved batch 14 of 174.\n",
            "Processed and saved batch 15 of 174.\n",
            "Processed and saved batch 16 of 174.\n",
            "Processed and saved batch 17 of 174.\n",
            "Processed and saved batch 18 of 174.\n",
            "Processed and saved batch 19 of 174.\n",
            "Processed and saved batch 20 of 174.\n",
            "Processed and saved batch 21 of 174.\n",
            "Processed and saved batch 22 of 174.\n",
            "Processed and saved batch 23 of 174.\n",
            "Processed and saved batch 24 of 174.\n",
            "Processed and saved batch 25 of 174.\n",
            "Processed and saved batch 26 of 174.\n",
            "Processed and saved batch 27 of 174.\n",
            "Processed and saved batch 28 of 174.\n",
            "Processed and saved batch 29 of 174.\n",
            "Processed and saved batch 30 of 174.\n",
            "Processed and saved batch 31 of 174.\n",
            "Processed and saved batch 32 of 174.\n",
            "Processed and saved batch 33 of 174.\n",
            "Processed and saved batch 34 of 174.\n",
            "Processed and saved batch 35 of 174.\n",
            "Processed and saved batch 36 of 174.\n",
            "Processed and saved batch 37 of 174.\n",
            "Processed and saved batch 38 of 174.\n",
            "Processed and saved batch 39 of 174.\n",
            "Processed and saved batch 40 of 174.\n",
            "Processed and saved batch 41 of 174.\n",
            "Processed and saved batch 42 of 174.\n",
            "Processed and saved batch 43 of 174.\n",
            "Processed and saved batch 44 of 174.\n",
            "Processed and saved batch 45 of 174.\n",
            "Processed and saved batch 46 of 174.\n",
            "Processed and saved batch 47 of 174.\n",
            "Processed and saved batch 48 of 174.\n",
            "Processed and saved batch 49 of 174.\n",
            "Processed and saved batch 50 of 174.\n",
            "Processed and saved batch 51 of 174.\n",
            "Processed and saved batch 52 of 174.\n",
            "Processed and saved batch 53 of 174.\n",
            "Processed and saved batch 54 of 174.\n",
            "Processed and saved batch 55 of 174.\n",
            "Processed and saved batch 56 of 174.\n",
            "Processed and saved batch 57 of 174.\n",
            "Processed and saved batch 58 of 174.\n",
            "Processed and saved batch 59 of 174.\n",
            "Processed and saved batch 60 of 174.\n",
            "Processed and saved batch 61 of 174.\n",
            "Processed and saved batch 62 of 174.\n",
            "Processed and saved batch 63 of 174.\n",
            "Processed and saved batch 64 of 174.\n",
            "Processed and saved batch 65 of 174.\n",
            "Processed and saved batch 66 of 174.\n",
            "Processed and saved batch 67 of 174.\n",
            "Processed and saved batch 68 of 174.\n",
            "Processed and saved batch 69 of 174.\n",
            "Processed and saved batch 70 of 174.\n",
            "Processed and saved batch 71 of 174.\n",
            "Processed and saved batch 72 of 174.\n",
            "Processed and saved batch 73 of 174.\n",
            "Processed and saved batch 74 of 174.\n",
            "Processed and saved batch 75 of 174.\n",
            "Processed and saved batch 76 of 174.\n",
            "Processed and saved batch 77 of 174.\n",
            "Processed and saved batch 78 of 174.\n",
            "Processed and saved batch 79 of 174.\n",
            "Processed and saved batch 80 of 174.\n",
            "Processed and saved batch 81 of 174.\n",
            "Processed and saved batch 82 of 174.\n",
            "Processed and saved batch 83 of 174.\n",
            "Processed and saved batch 84 of 174.\n",
            "Processed and saved batch 85 of 174.\n",
            "Processed and saved batch 86 of 174.\n",
            "Processed and saved batch 87 of 174.\n",
            "Processed and saved batch 88 of 174.\n",
            "Processed and saved batch 89 of 174.\n",
            "Processed and saved batch 90 of 174.\n",
            "Processed and saved batch 91 of 174.\n",
            "Processed and saved batch 92 of 174.\n",
            "Processed and saved batch 93 of 174.\n",
            "Processed and saved batch 94 of 174.\n",
            "Processed and saved batch 95 of 174.\n",
            "Processed and saved batch 96 of 174.\n",
            "Processed and saved batch 97 of 174.\n",
            "Processed and saved batch 98 of 174.\n",
            "Processed and saved batch 99 of 174.\n",
            "Processed and saved batch 100 of 174.\n",
            "Processed and saved batch 101 of 174.\n",
            "Processed and saved batch 102 of 174.\n",
            "Processed and saved batch 103 of 174.\n",
            "Processed and saved batch 104 of 174.\n",
            "Processed and saved batch 105 of 174.\n",
            "Processed and saved batch 106 of 174.\n",
            "Processed and saved batch 107 of 174.\n",
            "Processed and saved batch 108 of 174.\n",
            "Processed and saved batch 109 of 174.\n",
            "Processed and saved batch 110 of 174.\n",
            "Processed and saved batch 111 of 174.\n",
            "Processed and saved batch 112 of 174.\n",
            "Processed and saved batch 113 of 174.\n",
            "Processed and saved batch 114 of 174.\n",
            "Processed and saved batch 115 of 174.\n",
            "Processed and saved batch 116 of 174.\n",
            "Processed and saved batch 117 of 174.\n",
            "Processed and saved batch 118 of 174.\n",
            "Processed and saved batch 119 of 174.\n",
            "Processed and saved batch 120 of 174.\n",
            "Processed and saved batch 121 of 174.\n",
            "Processed and saved batch 122 of 174.\n",
            "Processed and saved batch 123 of 174.\n",
            "Processed and saved batch 124 of 174.\n",
            "Processed and saved batch 125 of 174.\n",
            "Processed and saved batch 126 of 174.\n",
            "Processed and saved batch 127 of 174.\n",
            "Processed and saved batch 128 of 174.\n",
            "Processed and saved batch 129 of 174.\n",
            "Processed and saved batch 130 of 174.\n",
            "Processed and saved batch 131 of 174.\n",
            "Processed and saved batch 132 of 174.\n",
            "Processed and saved batch 133 of 174.\n",
            "Processed and saved batch 134 of 174.\n",
            "Processed and saved batch 135 of 174.\n",
            "Processed and saved batch 136 of 174.\n",
            "Processed and saved batch 137 of 174.\n",
            "Processed and saved batch 138 of 174.\n",
            "Processed and saved batch 139 of 174.\n",
            "Processed and saved batch 140 of 174.\n",
            "Processed and saved batch 141 of 174.\n",
            "Processed and saved batch 142 of 174.\n",
            "Processed and saved batch 143 of 174.\n",
            "Processed and saved batch 144 of 174.\n",
            "Processed and saved batch 145 of 174.\n",
            "Processed and saved batch 146 of 174.\n",
            "Processed and saved batch 147 of 174.\n",
            "Processed and saved batch 148 of 174.\n",
            "Processed and saved batch 149 of 174.\n",
            "Processed and saved batch 150 of 174.\n",
            "Processed and saved batch 151 of 174.\n",
            "Processed and saved batch 152 of 174.\n",
            "Processed and saved batch 153 of 174.\n",
            "Processed and saved batch 154 of 174.\n",
            "Processed and saved batch 155 of 174.\n",
            "Processed and saved batch 156 of 174.\n",
            "Processed and saved batch 157 of 174.\n",
            "Processed and saved batch 158 of 174.\n",
            "Processed and saved batch 159 of 174.\n",
            "Processed and saved batch 160 of 174.\n",
            "Processed and saved batch 161 of 174.\n",
            "Processed and saved batch 162 of 174.\n",
            "Processed and saved batch 163 of 174.\n",
            "Processed and saved batch 164 of 174.\n",
            "Processed and saved batch 165 of 174.\n",
            "Processed and saved batch 166 of 174.\n",
            "Processed and saved batch 167 of 174.\n",
            "Processed and saved batch 168 of 174.\n",
            "Processed and saved batch 169 of 174.\n",
            "Processed and saved batch 170 of 174.\n",
            "Processed and saved batch 171 of 174.\n",
            "Processed and saved batch 172 of 174.\n",
            "Processed and saved batch 173 of 174.\n",
            "Processed and saved batch 174 of 174.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from all images in batches\n",
        "def extract_features_from_batches(batches):\n",
        "    \"\"\"Extract features for all images in the provided batches.\"\"\"\n",
        "    all_features = []\n",
        "    for batch in batches:\n",
        "        batch_features = []\n",
        "        for image in batch:\n",
        "            features = extract_minimal_features(image)\n",
        "            if features:  # Only include images where features were successfully extracted\n",
        "                batch_features.append(features)\n",
        "        all_features.extend(batch_features)  # Append all features from the batch\n",
        "    return all_features\n",
        "\n",
        "# Extract features for training and testing batches\n",
        "train_features = extract_features_from_batches(train_batches)\n",
        "test_features = extract_features_from_batches(test_batches)\n",
        "\n",
        "print(f\"Extracted features for {len(train_features)} training images and {len(test_features)} test images.\")\n"
      ],
      "metadata": {
        "id": "BFoqNxoyT1Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert features to a DataFrame\n",
        "train_features_df = pd.DataFrame(train_features)\n",
        "test_features_df = pd.DataFrame(test_features)\n",
        "\n",
        "# Check the DataFrame structure\n",
        "print(train_features_df.head())\n",
        "print(f\"Training Features Shape: {train_features_df.shape}\")\n",
        "print(f\"Test Features Shape: {test_features_df.shape}\")\n"
      ],
      "metadata": {
        "id": "NKEKHPzdFX1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Flatten label batches\n",
        "train_labels = np.hstack(train_label_batches)\n",
        "test_labels = np.hstack(test_label_batches)\n",
        "\n",
        "# Encode labels to numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_labels)\n",
        "y_test = label_encoder.transform(test_labels)\n",
        "\n",
        "# Check encoded labels\n",
        "print(f\"Encoded Training Labels: {y_train[:10]}\")\n",
        "print(f\"Classes: {label_encoder.classes_}\")\n"
      ],
      "metadata": {
        "id": "mO6ETXrnFcyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert DataFrames to NumPy arrays\n",
        "X_train = train_features_df.to_numpy()\n",
        "X_test = test_features_df.to_numpy()\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Random Forest Test Accuracy: {rf_accuracy * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_rf)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title(\"Confusion Matrix for Random Forest\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OlaNCvE_ThiZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}