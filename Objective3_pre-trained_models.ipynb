{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziFOSvVl_SUi"
      },
      "source": [
        "# **1) Downloading dataset and preprocessing it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPGjYxcR7Gy_",
        "outputId": "2bf201e4-6318-414a-9559-041f4496d1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "import os\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSge4RhC7LRE",
        "outputId": "e3f828f3-fb0f-4551-f044-232288e7c5c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle executable path: /usr/local/bin/kaggle\n"
          ]
        }
      ],
      "source": [
        "# Find the kaggle executable path\n",
        "kaggle_path = subprocess.check_output(['which', 'kaggle']).decode().strip()\n",
        "print(f\"Kaggle executable path: {kaggle_path}\")\n",
        "\n",
        "def download_kaggle_dataset(dataset_name, output_path):\n",
        "    \"\"\"Downloads a Kaggle dataset.\"\"\"\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    # Use the updated kaggle path\n",
        "    command = f\"{kaggle_path} datasets download -d {dataset_name} -p {output_path} --unzip\"\n",
        "    subprocess.run(command, shell=True, check=True)\n",
        "    print(f\"Dataset downloaded to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl9zAUbs7O5N",
        "outputId": "72545b04-6d81-442d-f08b-cf40055cfd18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: ./Datasets/asl-alphabet\n"
          ]
        }
      ],
      "source": [
        "dataset_name = \"grassknoted/asl-alphabet\"\n",
        "output_path = \"./Datasets/asl-alphabet\"\n",
        "download_kaggle_dataset(dataset_name, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5q_QQf1avkq",
        "outputId": "8db4c9d8-3d44-4600-a7a6-b1bd60ce799b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 69623 images belonging to 2 classes.\n",
            "Found 17405 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Paths to datasets\n",
        "asl_alphabet_path = \"./Datasets/asl-alphabet\"\n",
        "asl_numeric_path = \"./Datasets/american-sign-language-09az\"\n",
        "\n",
        "# Data Preprocessing Function\n",
        "def preprocess_data(data_dir, img_size=(224, 224)):\n",
        "    \"\"\"Loads and preprocesses image data.\"\"\"\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1.0/255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.2  # Use 20% of data for validation\n",
        "    )\n",
        "\n",
        "    train_gen = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training'\n",
        "    )\n",
        "    val_gen = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='validation'\n",
        "    )\n",
        "    return train_gen, val_gen\n",
        "\n",
        "# Preprocess both datasets\n",
        "train_gen, val_gen = preprocess_data(asl_alphabet_path)\n",
        "\n",
        "# Utility Function to Load and Modify Pre-Trained Models\n",
        "def load_model(base_model_fn, input_shape, num_classes):\n",
        "    base_model = base_model_fn(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
        "    base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    x = layers.Dense(512, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)  # Regularization to reduce overfitting\n",
        "    output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Train and Evaluate Function\n",
        "def train_and_evaluate_model(model, train_gen, val_gen, model_name, fine_tune=True, base_lr=1e-3, fine_tune_lr=1e-5, epochs=10):\n",
        "    if fine_tune:\n",
        "        model.layers[0].trainable = False  # Unfreeze base model\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=fine_tune_lr)\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=base_lr)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    print(f\"Training {model_name}...\")\n",
        "    history = model.fit(train_gen, validation_data=val_gen, epochs=epochs)\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    results = model.evaluate(val_gen)\n",
        "    print(f\"{model_name} - Loss: {results[0]}, Accuracy: {results[1]}\")\n",
        "\n",
        "    # Save Model\n",
        "    model.save(f\"{model_name}_fine_tuned.h5\")\n",
        "    return history, results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hgzG7GM_gsE"
      },
      "source": [
        "# **2) Starting in the objectives implementation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCulCJRWY5Od"
      },
      "source": [
        "**VGG 16 model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aREGOlU_0VJ",
        "outputId": "e4080b7c-249f-47db-d34d-190b90c129f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 69623 images belonging to 2 classes.\n",
            "Found 17405 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the dataset\n",
        "train_gen, val_gen = preprocess_data(asl_alphabet_path)\n",
        "\n",
        "# Define Early Stopping Callback\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dfhtUlPWerYs"
      },
      "outputs": [],
      "source": [
        "# VGG16 Model Setup\n",
        "def build_vgg16_model(input_shape, num_classes):\n",
        "    \"\"\"Build VGG16 model with reduced layers for fine-tuning.\"\"\"\n",
        "    vgg_base = VGG16(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
        "    vgg_base.trainable = False  # unFreeze the pre-trained VGG16 layers\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(vgg_base.output)\n",
        "    x = layers.Dense(50, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)  # Regularization to reduce overfitting\n",
        "    output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs=vgg_base.input, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7joRlfMUexkJ",
        "outputId": "72dfd6d9-35a0-40b9-918d-bc0853cbdea7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training VGG16...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1111s\u001b[0m 503ms/step - accuracy: 0.9707 - loss: 0.1132 - val_accuracy: 0.9997 - val_loss: 0.0029\n",
            "Epoch 2/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 490ms/step - accuracy: 0.9997 - loss: 0.0034 - val_accuracy: 0.9997 - val_loss: 0.0027\n",
            "Epoch 3/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1071s\u001b[0m 491ms/step - accuracy: 0.9997 - loss: 0.0031 - val_accuracy: 0.9997 - val_loss: 0.0028\n",
            "Epoch 4/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1151s\u001b[0m 513ms/step - accuracy: 0.9997 - loss: 0.0030 - val_accuracy: 0.9997 - val_loss: 0.0027\n",
            "Epoch 5/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1132s\u001b[0m 500ms/step - accuracy: 0.9996 - loss: 0.0032 - val_accuracy: 0.9997 - val_loss: 0.0027\n",
            "Epoch 6/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1076s\u001b[0m 488ms/step - accuracy: 0.9996 - loss: 0.0038 - val_accuracy: 0.9997 - val_loss: 0.0026\n",
            "Epoch 7/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1072s\u001b[0m 492ms/step - accuracy: 0.9997 - loss: 0.0033 - val_accuracy: 0.9997 - val_loss: 0.0027\n",
            "Epoch 8/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1097s\u001b[0m 489ms/step - accuracy: 0.9997 - loss: 0.0027 - val_accuracy: 0.9997 - val_loss: 0.0027\n",
            "Epoch 9/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1126s\u001b[0m 516ms/step - accuracy: 0.9998 - loss: 0.0021 - val_accuracy: 0.9997 - val_loss: 0.0026\n",
            "Epoch 10/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1110s\u001b[0m 493ms/step - accuracy: 0.9997 - loss: 0.0028 - val_accuracy: 0.9997 - val_loss: 0.0027\n",
            "Epoch 11/20\n",
            "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1073s\u001b[0m 492ms/step - accuracy: 0.9997 - loss: 0.0030 - val_accuracy: 0.9997 - val_loss: 0.0027\n",
            "Epoch 12/20\n",
            "\u001b[1m1965/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1:23\u001b[0m 395ms/step - accuracy: 0.9996 - loss: 0.0037"
          ]
        }
      ],
      "source": [
        "# Build and train VGG16 model\n",
        "vgg16_model = build_vgg16_model(input_shape=(224, 224, 3), num_classes=train_gen.num_classes)\n",
        "\n",
        "print(\"Training VGG16...\")\n",
        "vgg16_model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XvzauOQNe6B1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "6ab53b16-e48f-42f2-97ba-a50de2c0681d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vgg16_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-95976d5a7e30>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate VGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvgg16_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"VGG16 - Loss: {vgg16_results[0]}, Accuracy: {vgg16_results[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vgg16_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Evaluate VGG16\n",
        "vgg16_results = vgg16_model.evaluate(val_gen)\n",
        "print(f\"VGG16 - Loss: {vgg16_results[0]}, Accuracy: {vgg16_results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsJMPyD6YenR"
      },
      "source": [
        "**Inception V3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKVuCS_ffDqH"
      },
      "outputs": [],
      "source": [
        "# InceptionV3 Model Setup\n",
        "def build_inceptionv3_model(input_shape, num_classes):\n",
        "    \"\"\"Build InceptionV3 model with reduced layers for fine-tuning.\"\"\"\n",
        "    inception_base = InceptionV3(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
        "    inception_base.trainable = False  # Freeze the pre-trained InceptionV3 layers\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(inception_base.output)\n",
        "    x = layers.Dense(30, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)  # Regularization to reduce overfitting\n",
        "    output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs=inception_base.input, outputs=output)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpep2HYEfEjD"
      },
      "outputs": [],
      "source": [
        "# Build and train InceptionV3 model\n",
        "inceptionv3_model = build_inceptionv3_model(input_shape=(224, 224, 3), num_classes=train_gen.num_classes)\n",
        "\n",
        "print(\"Training InceptionV3...\")\n",
        "inceptionv3_model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UXU-DyFfJBC"
      },
      "outputs": [],
      "source": [
        "# Fine-tune InceptionV3 Model\n",
        "inceptionv3_model.layers[0].trainable = True  # Unfreeze last layers of InceptionV3\n",
        "inceptionv3_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                          loss=\"categorical_crossentropy\",\n",
        "                          metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"Fine-Tuning InceptionV3...\")\n",
        "inceptionv3_fine_tune_history = inceptionv3_model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG-TgA-NfJtI"
      },
      "outputs": [],
      "source": [
        "# Evaluate InceptionV3\n",
        "inceptionv3_results = inceptionv3_model.evaluate(val_gen)\n",
        "print(f\"InceptionV3 - Loss: {inceptionv3_results[0]}, Accuracy: {inceptionv3_results[1]}\")\n",
        "inceptionv3_model.save(\"InceptionV3_fine_tuned.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MobileNetV2**"
      ],
      "metadata": {
        "id": "aN__Ca8GgtLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocessing the dataset using TensorFlow's image_dataset_from_directory\n",
        "def preprocess_data(data_dir, img_size=(224, 224), batch_size=16):\n",
        "    \"\"\"Loads and preprocesses image data.\"\"\"\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        validation_split=0.2,\n",
        "        subset=\"training\",\n",
        "        seed=123,\n",
        "        image_size=img_size,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        validation_split=0.2,\n",
        "        subset=\"validation\",\n",
        "        seed=123,\n",
        "        image_size=img_size,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    # Get class_names before prefetching\n",
        "    class_names = train_ds.class_names\n",
        "\n",
        "    # Prefetch for performance\n",
        "    train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Return class_names along with datasets\n",
        "    return train_ds, val_ds, class_names"
      ],
      "metadata": {
        "id": "nz6Xjhwsgs1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the dataset\n",
        "train_ds, val_ds, class_names = preprocess_data(asl_alphabet_path, img_size=(224, 224), batch_size=16)"
      ],
      "metadata": {
        "id": "BSbVc4seg_Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the MobileNetV2 Model (ALL LAYERS FROZEN)\n",
        "def build_mobilenet_model(input_shape, num_classes):\n",
        "    \"\"\"Builds and compiles a MobileNetV2 model.\"\"\"\n",
        "    base_model = MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
        "    base_model.trainable = False  # Freeze the pre-trained layers\n",
        "\n",
        "    # Add custom layers\n",
        "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    x = layers.Dense(512, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)  # Regularization\n",
        "    output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    # Change loss function to 'sparse_categorical_crossentropy'\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Zk6bduc_hEDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = len(class_names)  # Use the extracted class_names\n",
        "mobilenet_model = build_mobilenet_model(input_shape, num_classes)\n",
        "\n",
        "# Early Stopping Callback\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the MobileNetV2 Model\n",
        "print(\"Training MobileNetV2...\")\n",
        "mobilenet_history = mobilenet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "xqvnMXCUhIHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tuning the MobileNetV2 Model (WITH LAYERS TRAINABLE)\n",
        "mobilenet_model.layers[0].trainable = True  # Unfreeze the base layers\n",
        "mobilenet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                        loss=\"sparse_categorical_crossentropy\", # Changed loss function\n",
        "                        metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"Fine-Tuning MobileNetV2...\")\n",
        "mobilenet_fine_tune_history = mobilenet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "jyg5Qx3DheNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "mobilenet_results = mobilenet_model.evaluate(val_ds)\n",
        "print(f\"MobileNetV2 - Loss: {mobilenet_results[0]}, Accuracy: {mobilenet_results[1]}\")"
      ],
      "metadata": {
        "id": "bN6faDc2hq8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Training and Validation Metrics\n",
        "def plot_metrics(history, model_name=\"Model\"):\n",
        "    \"\"\"Plots training and validation loss and accuracy.\"\"\"\n",
        "    train_losses = history.history['loss']\n",
        "    val_losses = history.history['val_loss']\n",
        "    train_accuracies = history.history['accuracy']\n",
        "    val_accuracies = history.history['val_accuracy']\n",
        "\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Plot Loss Curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
        "    plt.plot(epochs, val_losses, label=\"Validation Loss\", color='orange')\n",
        "    plt.title(f\"{model_name} - Loss Curve\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Accuracy Curves\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='blue')\n",
        "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\", color='orange')\n",
        "    plt.title(f\"{model_name} - Accuracy Curve\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the metrics for the fine-tuned model\n",
        "plot_metrics(mobilenet_fine_tune_history, model_name=\"MobileNetV2\")"
      ],
      "metadata": {
        "id": "y2_x5mm2hlbT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}